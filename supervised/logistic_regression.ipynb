{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "95bbd71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "#\n",
    "# Logistic Regression - Discriminative Classifier\n",
    "#\n",
    "############################################################\n",
    "#\n",
    "# Logistic Regression directly models P(Y | X) without making assumptions about P(X | Y)\n",
    "# Uses linear decision boundary with probabilistic output via sigmoid/softmax\n",
    "#\n",
    "# Binary case:\n",
    "# z = w路x + b (linear score)\n",
    "# P(Y=1 | X) = sigmoid(z) = 1 / (1 + exp(-z))\n",
    "# Decision boundary: w路x + b = 0\n",
    "#\n",
    "# Loss function: Binary Cross Entropy (Log Loss)\n",
    "# L = -mean(y*log(p) + (1-y)*log(1-p))\n",
    "# Penalizes confident wrong predictions heavily\n",
    "# Gradient descent minimizes average log loss over training data\n",
    "#\n",
    "# Multiclass case:\n",
    "# Scores: z_k = w_k路x + b_k for each class k\n",
    "# P(Y=k | X) = softmax(z)_k = exp(z_k) / sum(exp(z_j))\n",
    "# Loss function: Categorical Cross Entropy\n",
    "# L = -mean(sum(y_k * log(p_k)))\n",
    "#\n",
    "# Training:\n",
    "# Gradient descent updates weights based on prediction error\n",
    "# dw = (1/n) * X^T * (y_pred - y_true) + regularization_term\n",
    "# L2 regularization adds lambda*||w||^2 to loss to prevent overfitting\n",
    "# Lipschitz constant L determines safe step size: step_size = 1/L\n",
    "# L accounts for data curvature and regularization strength\n",
    "#\n",
    "# Key properties:\n",
    "# Makes no distributional assumptions about features\n",
    "# Decision boundary is linear in feature space\n",
    "# Output is calibrated probability\n",
    "# Training finds global optimum (convex loss)\n",
    "# Sensitive to feature scaling\n",
    "#\n",
    "# Known limitations\n",
    "# Stopping is based on a fixed iteration count, not a convergence test\n",
    "# No explicit train/validation split or evaluation metrics beyond basic checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1021cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary logistic regression setup (data + helper)\n",
    "X = np.array([[1.0, 1.0], [1.2, 0.8], [0.8, 1.3], [1.1, 1.4],\n",
    "              [3.0, 3.1], [2.8, 2.9], [3.2, 2.7], [3.1, 3.3]])\n",
    "Y = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    out = np.zeros_like(z)\n",
    "    pos = z >= 0\n",
    "    neg = ~pos\n",
    "    out[pos] = 1 / (1 + np.exp(-z[pos]))\n",
    "    exp_z = np.exp(z[neg])\n",
    "    out[neg] = exp_z / (1 + exp_z)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "32337fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary logistic regression (gradient descent)\n",
    "# - Linear score z = w路x + b, sigmoid maps z to probability\n",
    "# - We update weights to minimize average log-loss over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6bd74260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary predictions (0/1):\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Final Binary Cross Entropy loss: 7.6858\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(Y)\n",
    "n_samples, n_features = np.shape(X)\n",
    "X0 = X[Y == 0]\n",
    "X1 = X[Y == 1]\n",
    "P_Y1 = len(X1) / len(X)\n",
    "\n",
    "mu0 = np.mean(X0, axis=0)\n",
    "mu1 = np.mean(X1, axis=0)\n",
    "var = (np.var(X, axis=0) / 2) + 1e-9\n",
    "\n",
    "# Weights derived from Gaussian Naive Bayes assumptions\n",
    "w0 = np.log((1 - P_Y1) / P_Y1) + np.sum((mu1**2 - mu0**2) / (2 * var))\n",
    "w1 = (mu0 - mu1) / var\n",
    "X_new = np.array([[1.0, 1.0], [1.2, 0.8], [0.8, 1.3], [1.1, 1.4],\n",
    "                  [3.0, 3.1], [2.8, 2.9], [3.2, 2.7], [3.1, 3.3]])\n",
    "\n",
    "print(\"Binary predictions (0/1):\")\n",
    "for i in X_new:\n",
    "    if (w0 + np.dot(w1, i)) > 0:\n",
    "        print(0)\n",
    "    else:\n",
    "        print(1)\n",
    "\n",
    "z = w0 + np.dot(X, w1)\n",
    "y_pred = sigmoid(z)\n",
    "\n",
    "loss = np.mean(Y * np.log(y_pred + 1e-9) + (1 - Y) * np.log(1 - y_pred + 1e-9))\n",
    "print(f\"Final Binary Cross Entropy loss: {-loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eb680ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary LR predictions (0/1):\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Final Binary Cross Entropy loss: 0.1383\n"
     ]
    }
   ],
   "source": [
    "#Adding L2 regularization over weights and Lipschitz for finding optimal step size\n",
    "#L is effectively how curved/sharp the loss surface can be\n",
    "#Since loss now is loss_data  + loss_reg (reg_strength*w**2) we are adding a parabola on top of it so we need to accomodate the required step size to check that \n",
    "cycles = 100\n",
    "reg_strength =  0.00001\n",
    "L_constant = (1/(4*n_samples)) * np.linalg.norm(X.T @ X, ord=2) + (reg_strength)\n",
    "step_size = 1/L_constant\n",
    "w0 = 0.0\n",
    "w1 = np.zeros(n_features)\n",
    "for i in range(cycles):\n",
    "    z = np.dot(X, w1) + w0\n",
    "    y_pred = sigmoid(z)\n",
    "    err = y_pred-Y\n",
    "    dw1 = (1 / n_samples) * np.dot(X.T, err) + (reg_strength)*w1\n",
    "    dw0 = (1 / n_samples) * np.sum(err)\n",
    "    w0 = w0 - step_size * dw0\n",
    "    w1 = w1 - step_size * dw1\n",
    "\n",
    "X_new = np.array([[1.0, 1.0], [1.2, 0.8], [0.8, 1.3], [1.1, 1.4],\n",
    "                  [3.0, 3.1], [2.8, 2.9], [3.2, 2.7], [3.1, 3.3]])\n",
    "\n",
    "print(\"binary LR predictions (0/1):\")\n",
    "for x in X_new:\n",
    "    z = np.dot(w1, x) + w0\n",
    "    print(1 if z > 0 else 0)\n",
    "loss = np.mean(Y * np.log(y_pred + 1e-9) + (1 - Y) * np.log(1 - y_pred + 1e-9))\n",
    "print(f\"Final Binary Cross Entropy loss: {-loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0febc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax class probabilities:\n",
      "[[0.61668664 0.33825478 0.04505858]\n",
      " [0.87722163 0.08435287 0.0384255 ]\n",
      " [0.15312848 0.82144171 0.02542982]\n",
      " [0.21419537 0.7288828  0.05692182]\n",
      " [0.00493907 0.29593047 0.69913046]\n",
      " [0.00917108 0.36269698 0.62813194]\n",
      " [0.01988921 0.07106701 0.90904378]\n",
      " [0.00247077 0.31242136 0.68510787]]\n",
      "multiclass LR predictions:\n",
      "[0 0 1 1 2 2 2 2]\n",
      "Final Binary Cross Entropy loss: 0.4447\n"
     ]
    }
   ],
   "source": [
    "# Multiclass logistic regression (softmax)\n",
    "# One linear score per class\n",
    "# Softmax turns scores into a probability distribution over classes\n",
    "# rained with cross-entropy loss via gradient descent\n",
    "\n",
    "cycles = 1000\n",
    "X = np.array([[1.0, 1.0], [1.2, 0.8], [0.8, 1.3], [1.1, 1.4],\n",
    "              [3.0, 3.1], [2.8, 2.9], [3.2, 2.7], [3.1, 3.3]])\n",
    "Y = np.array([0, 0, 1, 1, 1, 2, 2, 2])\n",
    "classes = np.unique(Y)\n",
    "w0_mult = np.zeros((1, len(classes)))\n",
    "w_mult = np.zeros((n_features, len(classes)))\n",
    "step_size = 0.1\n",
    "Y_multi = np.zeros((n_samples, len(classes)))\n",
    "for idx, c in enumerate(classes):\n",
    "    Y_multi[Y == c, idx] = 1\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "for i in range(cycles):\n",
    "    scores = np.dot(X, w_mult) + w0_mult\n",
    "    dominance = softmax(scores)\n",
    "    err = dominance - Y_multi\n",
    "    dw1 = (1 / n_samples) * np.dot(X.T, err)\n",
    "    dw0 = (1 / n_samples) * np.sum(err, axis=0, keepdims=True)\n",
    "    w0_mult -= step_size * dw0\n",
    "    w_mult -= step_size * dw1\n",
    "\n",
    "print(\"softmax class probabilities:\")\n",
    "print(dominance)\n",
    "print(\"multiclass LR predictions:\")\n",
    "print(np.argmax(dominance, axis=1))\n",
    "loss = np.mean(Y * np.log(y_pred + 1e-9) + (1 - Y) * np.log(1 - y_pred + 1e-9))\n",
    "print(f\"Final Binary Cross Entropy loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
