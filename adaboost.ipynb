{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# ADABOOST \n",
    "#################################################\n",
    "#\n",
    "# we want a binary classifier of the form y_cap(x) = sign(F(x))\n",
    "# F(x) is represented as a linear combination of weak classifiers (simple decision rules) with some weight\n",
    "# F(x) = sum(alpha_i * h_i(x)) -> h_i(x) is the ith decision rule , alpha_i is its weight of how much it contributes to F(x) , h(x) -> {-1,1}\n",
    "# Margin of point i -> gamma_i = y_i * F(x_i) - correct if > 0 else < 0\n",
    "# Adaboost chooses exponential margin loss -> L(F) = sum(exp(-gamma_i))\n",
    "# Define the weight of each data point as how much they contribute to the total loss -> w_i = exp(-y_i*F_t-1(x_i))   (at t iteration we would have the t-1 th version of F)\n",
    "# Now add another weak rule alpha_t*h_t -> F_t = F_t-1 + alpha_t*h_t\n",
    "# L = sum (w_t_i * (-alpha*y_i*x_i))  -> pick h_t from hypothesis space such that this new loss is minimum\n",
    "# its weighted error is e_t = sum(w_t_i * 1[h(x_i) != y_i])  -> 1[] is 0-1 loss - count of wrong predictions * weight (here)\n",
    "# alpha_t by minimisation = 1/2 * log (1-e_t/e_t)\n",
    "# update F and thereby loss weights implicitly of the new F\n",
    "#\n",
    "# limitations / assumptions:\n",
    "# assumes weak learners perform slightly better than random\n",
    "# highly sensitive to label noise near the decision boundary\n",
    "# optimizes exponential loss, not robustness\n",
    "# can overfit by focusing excessively on hard or noisy points\n",
    "# accuracy can look perfect even when margins deteriorate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([])\n",
    "Y = np.array([])\n",
    "\n",
    "def adaboost(X,Y,T):\n",
    "\n",
    "    n_samples,n_features = X.shape\n",
    "    w = np.ones(n_samples)/n_samples\n",
    "    h_arr = []\n",
    "    alpha_arr = []\n",
    "\n",
    "    def fit_weak_learner(X,Y,w):\n",
    "        opt_err = np.inf\n",
    "        opt_h = None\n",
    "        for feature in range(n_features):\n",
    "            vals = np.unique(X[:, feature])\n",
    "            thresholds = (vals[:-1] + vals[1:])/2\n",
    "            for threshold in thresholds:\n",
    "                for clas in [-1,1]:\n",
    "                    pred = np.ones(n_samples) * (-clas)\n",
    "                    pred[X[:,feature]<=threshold] = clas\n",
    "                    error = np.sum(w[pred != Y])\n",
    "                    if error<opt_err:\n",
    "                        opt_err = error\n",
    "                        opt_h = {\"feature\":feature,\"threshold\":threshold,\"class\":clas}\n",
    " \n",
    "\n",
    "        pred = np.ones(n_samples)*(-opt_h['class'])\n",
    "        pred[X[:, opt_h['feature']] <= opt_h['threshold']] = opt_h['class']\n",
    "        return opt_h,pred\n",
    "    \n",
    "\n",
    "    for t in range(T):\n",
    "        \n",
    "        h,pred = fit_weak_learner(X,Y,w)\n",
    "        epsilon = np.sum(w[pred != Y]) / np.sum(w)\n",
    "        epsilon = np.sum(w[pred != Y]) / np.sum(w)\n",
    "        # When adaboost finds a weak learner with zero weighted error\n",
    "        # the exponential loss is already minimized -> the optimal step is infinite -> and the algorithm must add that learner once and stop\n",
    "        if epsilon < 1e-12:\n",
    "            alpha = 0.5 * np.log((1 - 1e-12) / 1e-12)\n",
    "            h_arr.append(h)\n",
    "            alpha_arr.append(alpha)\n",
    "            break\n",
    "        else:\n",
    "            alpha = 0.5 * np.log((1 - epsilon) / epsilon)\n",
    "            h_arr.append(h)\n",
    "            alpha_arr.append(alpha)\n",
    "            w *= np.exp(-alpha * Y * pred)\n",
    "\n",
    "\n",
    "    return h_arr,alpha_arr\n",
    "def predict(X_test, h_arr, alpha_arr):\n",
    "    predictions = np.zeros(len(X_test))\n",
    "    for h, alpha in zip(h_arr, alpha_arr):\n",
    "        pred = np.ones(len(X_test)) * (-h['class'])\n",
    "        pred[X_test[:, h['feature']] <= h['threshold']] = h['class']\n",
    "        predictions += alpha * pred\n",
    "    final_predictions = np.sign(predictions)\n",
    "    final_predictions[final_predictions == 0] = -1\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044d337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== CLEAN SEPARABLE ====\n",
      "Alphas: ['13.8155']\n",
      "Predictions: [-1 -1 -1 -1  1  1  1  1]\n",
      "True Labels: [-1 -1 -1 -1  1  1  1  1]\n",
      "Accuracy: 100.0%\n",
      "Expected: Perfect separation, should achieve 100% accuracy\n",
      "\n",
      "==== XOR ====\n",
      "Alphas: ['0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000']\n",
      "Predictions: [-1 -1 -1 -1]\n",
      "True Labels: [-1  1  1 -1]\n",
      "Accuracy: 50.0%\n",
      "Expected: XOR is not linearly separable, accuracy ~50% (random guessing)\n",
      "\n",
      "==== SINGLE LABEL NOISE ====\n",
      "Alphas: ['0.9730', '0.8959', '0.8047', '0.7753', '0.7520', '0.7408', '0.7332', '0.7289', '0.7261', '0.7245']\n",
      "Predictions: [-1 -1 -1 -1  1  1  1 -1]\n",
      "True Labels: [-1 -1 -1 -1  1  1  1 -1]\n",
      "Accuracy: 100.0%\n",
      "Expected: AdaBoost may overfit to the noisy label, achieving high training accuracy\n",
      "\n",
      "==== OVERLAP / NO SEPARATION ====\n",
      "Alphas: ['0.9730', '1.2825', '0.6020', '0.7753', '0.7027', '0.7293', '0.7190', '0.7229', '0.7214', '0.7220']\n",
      "Predictions: [-1 -1 -1  1  1  1  1  1]\n",
      "True Labels: [-1 -1 -1  1  1  1  1  1]\n",
      "Accuracy: 100.0%\n",
      "Expected: Classes overlap, moderate accuracy, alphas may decrease over iterations\n",
      "\n",
      "==== DEGENERATE FEATURE ====\n",
      "Alphas: ['13.8155']\n",
      "Predictions: [-1 -1 -1 -1  1  1  1  1]\n",
      "True Labels: [-1 -1 -1 -1  1  1  1  1]\n",
      "Accuracy: 100.0%\n",
      "Expected: Feature 0 is useless (constant), only feature 1 helps, should still achieve 100%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [1.0, 1.0],\n",
    "    [1.2, 1.1],\n",
    "    [0.9, 1.2],\n",
    "    [1.1, 0.9],\n",
    "    [3.0, 3.0],\n",
    "    [3.1, 2.9],\n",
    "    [2.9, 3.1],\n",
    "    [3.2, 3.0]\n",
    "])\n",
    "\n",
    "Y = np.array([-1, -1, -1, -1, 1, 1, 1, 1])\n",
    "\n",
    "T = 5\n",
    "\n",
    "h_arr, alpha_arr = adaboost(X, Y, T)\n",
    "predictions = predict(X, h_arr, alpha_arr)\n",
    "accuracy = np.mean(predictions == Y)\n",
    "\n",
    "print(\"==== CLEAN SEPARABLE ====\")\n",
    "print(\"Alphas:\", [f\"{a:.4f}\" for a in alpha_arr])\n",
    "print(\"Predictions:\", predictions.astype(int))\n",
    "print(\"True Labels:\", Y)\n",
    "print(f\"Accuracy: {accuracy*100:.1f}%\")\n",
    "print(\"Expected: Perfect separation, should achieve 100% accuracy\\n\")\n",
    "X = np.array([\n",
    "    [0.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 0.0],\n",
    "    [1.0, 1.0]\n",
    "])\n",
    "\n",
    "Y = np.array([-1, 1, 1, -1])\n",
    "\n",
    "T = 10\n",
    "\n",
    "h_arr, alpha_arr = adaboost(X, Y, T)\n",
    "predictions = predict(X, h_arr, alpha_arr)\n",
    "accuracy = np.mean(predictions == Y)\n",
    "\n",
    "print(\"==== XOR ====\")\n",
    "print(\"Alphas:\", [f\"{a:.4f}\" for a in alpha_arr])\n",
    "print(\"Predictions:\", predictions.astype(int))\n",
    "print(\"True Labels:\", Y)\n",
    "print(f\"Accuracy: {accuracy*100:.1f}%\")\n",
    "print(\"Expected: XOR is not linearly separable, accuracy ~50% (random guessing)\\n\")\n",
    "X = np.array([\n",
    "    [1.0, 1.0],\n",
    "    [1.1, 1.0],\n",
    "    [0.9, 1.1],\n",
    "    [1.2, 0.9],\n",
    "    [3.0, 3.0],\n",
    "    [3.1, 2.9],\n",
    "    [2.9, 3.1],\n",
    "    [3.2, 3.0]\n",
    "])\n",
    "Y = np.array([-1, -1, -1, -1, 1, 1, 1, -1])\n",
    "\n",
    "T = 20\n",
    "\n",
    "h_arr, alpha_arr = adaboost(X, Y, T)\n",
    "predictions = predict(X, h_arr, alpha_arr)\n",
    "accuracy = np.mean(predictions == Y)\n",
    "\n",
    "print(\"==== SINGLE LABEL NOISE ====\")\n",
    "print(\"Alphas:\", [f\"{a:.4f}\" for a in alpha_arr[:10]]) \n",
    "print(\"Predictions:\", predictions.astype(int))\n",
    "print(\"True Labels:\", Y)\n",
    "print(f\"Accuracy: {accuracy*100:.1f}%\")\n",
    "print(\"Expected: AdaBoost may overfit to the noisy label, achieving high training accuracy\\n\")\n",
    "X = np.array([\n",
    "    [0.0, 0.0], [0.1, 0.1], [0.2, 0.2],  # Class -1\n",
    "    [0.15, 0.15], [0.25, 0.25],           # Class 1 (overlapping)\n",
    "    [1.0, 1.0], [1.1, 1.1], [1.2, 1.2]    # Class 1\n",
    "])\n",
    "Y = np.array([-1, -1, -1, 1, 1, 1, 1, 1])\n",
    "T = 30\n",
    "\n",
    "h_arr, alpha_arr = adaboost(X, Y, T)\n",
    "predictions = predict(X, h_arr, alpha_arr)\n",
    "accuracy = np.mean(predictions == Y)\n",
    "\n",
    "print(\"==== OVERLAP / NO SEPARATION ====\")\n",
    "print(\"Alphas:\", [f\"{a:.4f}\" for a in alpha_arr[:10]]) \n",
    "print(\"Predictions:\", predictions.astype(int))\n",
    "print(\"True Labels:\", Y)\n",
    "print(f\"Accuracy: {accuracy*100:.1f}%\")\n",
    "print(\"Expected: Classes overlap, moderate accuracy, alphas may decrease over iterations\\n\")\n",
    "X = np.array([\n",
    "    [1.0, 0.0],\n",
    "    [1.0, 0.1],\n",
    "    [1.0, 0.2],\n",
    "    [1.0, 0.3],\n",
    "    [1.0, 1.0],\n",
    "    [1.0, 1.1],\n",
    "    [1.0, 1.2],\n",
    "    [1.0, 1.3]\n",
    "])\n",
    "\n",
    "Y = np.array([-1, -1, -1, -1, 1, 1, 1, 1])\n",
    "\n",
    "T = 5\n",
    "\n",
    "h_arr, alpha_arr = adaboost(X, Y, T)\n",
    "predictions = predict(X, h_arr, alpha_arr)\n",
    "accuracy = np.mean(predictions == Y)\n",
    "\n",
    "print(\"==== DEGENERATE FEATURE ====\")\n",
    "print(\"Alphas:\", [f\"{a:.4f}\" for a in alpha_arr])\n",
    "print(\"Predictions:\", predictions.astype(int))\n",
    "print(\"True Labels:\", Y)\n",
    "print(f\"Accuracy: {accuracy*100:.1f}%\")\n",
    "print(\"Expected: Feature 0 is useless (constant), only feature 1 helps, should still achieve 100%\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
