{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d24a9f42",
   "metadata": {},
   "source": [
    "# Logistic Regression from Scratch\n",
    "\n",
    "This notebook implements binary and multiclass logistic regression from scratch.\n",
    "The focus is on understanding assumptions, loss functions, and optimization behavior,\n",
    "not performance or abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "95bbd71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c03a9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression: discriminative model\n",
    "# - Directly models P(y | x), not the data distribution P(x | y)\n",
    "# - Uses a linear score in x; sigmoid/softmax only map scores to (0, 1)\n",
    "# - Trains by minimizing log-loss (penalizes confident wrong predictions)\n",
    "# - Makes no explicit assumptions about how x is generated\n",
    "\n",
    "# Loss intuition\n",
    "# - For each sample, we want the predicted probability of the true class to be high\n",
    "# - Log-loss is just the negative log-likelihood over all samples\n",
    "# - Using log turns products into sums and magnifies large errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1021cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary logistic regression setup (data + helper)\n",
    "X = np.array([[1.0, 1.0], [1.2, 0.8], [0.8, 1.3], [1.1, 1.4],\n",
    "              [3.0, 3.1], [2.8, 2.9], [3.2, 2.7], [3.1, 3.3]])\n",
    "Y = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    out = np.zeros_like(z)\n",
    "    pos = z >= 0\n",
    "    neg = ~pos\n",
    "    out[pos] = 1 / (1 + np.exp(-z[pos]))\n",
    "    exp_z = np.exp(z[neg])\n",
    "    out[neg] = exp_z / (1 + exp_z)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "32337fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary logistic regression (gradient descent)\n",
    "# - Linear score z = wÂ·x + b, sigmoid maps z to probability\n",
    "# - We update weights to minimize average log-loss over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6bd74260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary predictions (0/1):\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Final Binary Cross Entropy loss: 7.6858\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(Y)\n",
    "n_samples, n_features = np.shape(X)\n",
    "X0 = X[Y == 0]\n",
    "X1 = X[Y == 1]\n",
    "P_Y1 = len(X1) / len(X)\n",
    "\n",
    "mu0 = np.mean(X0, axis=0)\n",
    "mu1 = np.mean(X1, axis=0)\n",
    "var = (np.var(X, axis=0) / 2) + 1e-9\n",
    "\n",
    "# Weights derived from Gaussian Naive Bayes assumptions\n",
    "w0 = np.log((1 - P_Y1) / P_Y1) + np.sum((mu1**2 - mu0**2) / (2 * var))\n",
    "w1 = (mu0 - mu1) / var\n",
    "X_new = np.array([[1.0, 1.0], [1.2, 0.8], [0.8, 1.3], [1.1, 1.4],\n",
    "                  [3.0, 3.1], [2.8, 2.9], [3.2, 2.7], [3.1, 3.3]])\n",
    "\n",
    "print(\"Binary predictions (0/1):\")\n",
    "for i in X_new:\n",
    "    if (w0 + np.dot(w1, i)) > 0:\n",
    "        print(0)\n",
    "    else:\n",
    "        print(1)\n",
    "\n",
    "z = w0 + np.dot(X, w1)\n",
    "y_pred = sigmoid(z)\n",
    "\n",
    "loss = np.mean(Y * np.log(y_pred + 1e-9) + (1 - Y) * np.log(1 - y_pred + 1e-9))\n",
    "print(f\"Final Binary Cross Entropy loss: {-loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eb680ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary LR predictions (0/1):\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Final Binary Cross Entropy loss: 0.1383\n"
     ]
    }
   ],
   "source": [
    "#Adding L2 regularization over weights and Lipschitz for finding optimal step size\n",
    "#L is effectively how curved/sharp the loss surface can be\n",
    "#Since loss now is loss_data  + loss_reg (reg_strength*w**2) we are adding a parabola on top of it so we need to accomodate the required step size to check that \n",
    "cycles = 100\n",
    "reg_strength =  0.00001\n",
    "L_constant = (1/(4*n_samples)) * np.linalg.norm(X.T @ X, ord=2) + (reg_strength)\n",
    "step_size = 1/L_constant\n",
    "w0 = 0.0\n",
    "w1 = np.zeros(n_features)\n",
    "for i in range(cycles):\n",
    "    z = np.dot(X, w1) + w0\n",
    "    y_pred = sigmoid(z)\n",
    "    err = y_pred-Y\n",
    "    dw1 = (1 / n_samples) * np.dot(X.T, err) + (reg_strength)*w1\n",
    "    dw0 = (1 / n_samples) * np.sum(err)\n",
    "    w0 = w0 - step_size * dw0\n",
    "    w1 = w1 - step_size * dw1\n",
    "\n",
    "X_new = np.array([[1.0, 1.0], [1.2, 0.8], [0.8, 1.3], [1.1, 1.4],\n",
    "                  [3.0, 3.1], [2.8, 2.9], [3.2, 2.7], [3.1, 3.3]])\n",
    "\n",
    "print(\"binary LR predictions (0/1):\")\n",
    "for x in X_new:\n",
    "    z = np.dot(w1, x) + w0\n",
    "    print(1 if z > 0 else 0)\n",
    "loss = np.mean(Y * np.log(y_pred + 1e-9) + (1 - Y) * np.log(1 - y_pred + 1e-9))\n",
    "print(f\"Final Binary Cross Entropy loss: {-loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cb0febc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax class probabilities:\n",
      "[[0.61668664 0.33825478 0.04505858]\n",
      " [0.87722163 0.08435287 0.0384255 ]\n",
      " [0.15312848 0.82144171 0.02542982]\n",
      " [0.21419537 0.7288828  0.05692182]\n",
      " [0.00493907 0.29593047 0.69913046]\n",
      " [0.00917108 0.36269698 0.62813194]\n",
      " [0.01988921 0.07106701 0.90904378]\n",
      " [0.00247077 0.31242136 0.68510787]]\n",
      "multiclass LR predictions:\n",
      "[0 0 1 1 2 2 2 2]\n",
      "Final Binary Cross Entropy loss: 0.4447\n"
     ]
    }
   ],
   "source": [
    "# Multiclass logistic regression (softmax)\n",
    "# - One linear score per class\n",
    "# - Softmax turns scores into a probability distribution over classes\n",
    "# - Trained with cross-entropy loss via gradient descent\n",
    "\n",
    "cycles = 1000\n",
    "X = np.array([[1.0, 1.0], [1.2, 0.8], [0.8, 1.3], [1.1, 1.4],\n",
    "              [3.0, 3.1], [2.8, 2.9], [3.2, 2.7], [3.1, 3.3]])\n",
    "Y = np.array([0, 0, 1, 1, 1, 2, 2, 2])\n",
    "classes = np.unique(Y)\n",
    "w0_mult = np.zeros((1, len(classes)))\n",
    "w_mult = np.zeros((n_features, len(classes)))\n",
    "step_size = 0.1\n",
    "Y_multi = np.zeros((n_samples, len(classes)))\n",
    "for idx, c in enumerate(classes):\n",
    "    Y_multi[Y == c, idx] = 1\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "for i in range(cycles):\n",
    "    scores = np.dot(X, w_mult) + w0_mult\n",
    "    dominance = softmax(scores)\n",
    "    err = dominance - Y_multi\n",
    "    dw1 = (1 / n_samples) * np.dot(X.T, err)\n",
    "    dw0 = (1 / n_samples) * np.sum(err, axis=0, keepdims=True)\n",
    "    w0_mult -= step_size * dw0\n",
    "    w_mult -= step_size * dw1\n",
    "\n",
    "print(\"softmax class probabilities:\")\n",
    "print(dominance)\n",
    "print(\"multiclass LR predictions:\")\n",
    "print(np.argmax(dominance, axis=1))\n",
    "loss = np.mean(Y * np.log(y_pred + 1e-9) + (1 - Y) * np.log(1 - y_pred + 1e-9))\n",
    "print(f\"Final Binary Cross Entropy loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bc015c",
   "metadata": {},
   "source": [
    "## Known limitations\n",
    "\n",
    "- Fixed learning rate (no scheduling or adaptivity).\n",
    "- ~~No regularization (L1/L2 not implemented).~~\n",
    "- Stopping is based on a fixed iteration count, not a convergence test.\n",
    "- No explicit train/validation split or evaluation metrics beyond basic sanity checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16395aa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
