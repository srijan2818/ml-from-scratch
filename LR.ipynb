{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d24a9f42",
   "metadata": {},
   "source": [
    "# Logistic Regression from Scratch\n",
    "\n",
    "This notebook implements binary and multiclass logistic regression from scratch.\n",
    "The focus is on understanding assumptions, loss functions, and optimization behavior,\n",
    "not performance or abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bbd71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression: discriminative model\n",
    "# - Directly models P(y | x), not the data distribution P(x | y)\n",
    "# - Uses a linear score in x; sigmoid/softmax only map scores to (0, 1)\n",
    "# - Trains by minimizing log-loss (penalizes confident wrong predictions)\n",
    "# - Makes no explicit assumptions about how x is generated\n",
    "\n",
    "# Loss intuition\n",
    "# - For each sample, we want the predicted probability of the true class to be high\n",
    "# - Log-loss is just the negative log-likelihood over all samples\n",
    "# - Using log turns products into sums and magnifies large errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1021cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary logistic regression setup (data + helper)\n",
    "X = np.array([[1.0, 1.0], [1.2, 0.8], [0.8, 1.3], [1.1, 1.4],\n",
    "              [3.0, 3.1], [2.8, 2.9], [3.2, 2.7], [3.1, 3.3]])\n",
    "Y = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32337fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary logistic regression (gradient descent)\n",
    "# - Linear score z = wÂ·x + b, sigmoid maps z to probability\n",
    "# - We update weights to minimize average log-loss over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd74260",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycles = 200  # fixed number of gradient steps\n",
    "step_size = 0.1\n",
    "w0 = 0.0\n",
    "w1 = np.zeros(n_features)\n",
    "\n",
    "for i in range(cycles):\n",
    "    z = np.dot(X, w1) + w0\n",
    "    y_pred = sigmoid(z)\n",
    "    err = Y - y_pred\n",
    "    dw1 = (1 / n_samples) * np.dot(X.T, err)\n",
    "    dw0 = (1 / n_samples) * np.sum(err)\n",
    "    w0 -= step_size * dw0\n",
    "    w1 -= step_size * dw1\n",
    "    if i % 50 == 0:\n",
    "        loss = -np.mean(Y * np.log(y_pred + 1e-9) + (1 - Y) * np.log(1 - y_pred + 1e-9))\n",
    "        print(f\"iter {i}, loss = {loss:.4f}\")\n",
    "\n",
    "X_new = np.array([[1.0, 1.0], [1.2, 0.8], [0.8, 1.3], [1.1, 1.4],\n",
    "                  [3.0, 3.1], [2.8, 2.9], [3.2, 2.7], [3.1, 3.3]])\n",
    "\n",
    "print(\"binary LR predictions (0/1):\")\n",
    "for x in X_new:\n",
    "    z = np.dot(x, w1) + w0\n",
    "    p1 = sigmoid(z)\n",
    "    print(1 if p1 >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0febc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass logistic regression (softmax)\n",
    "# - One linear score per class\n",
    "# - Softmax turns scores into a probability distribution over classes\n",
    "# - Trained with cross-entropy loss via gradient descent\n",
    "\n",
    "cycles = 1000\n",
    "X = np.array([[1.0, 1.0], [1.2, 0.8], [0.8, 1.3], [1.1, 1.4],\n",
    "              [3.0, 3.1], [2.8, 2.9], [3.2, 2.7], [3.1, 3.3]])\n",
    "Y = np.array([0, 0, 1, 1, 1, 2, 2, 2])\n",
    "classes = np.unique(Y)\n",
    "w0_mult = np.zeros((1, len(classes)))\n",
    "w_mult = np.zeros((n_features, len(classes)))\n",
    "step_size = 0.1\n",
    "Y_multi = np.zeros((n_samples, len(classes)))\n",
    "for idx, c in enumerate(classes):\n",
    "    Y_multi[Y == c, idx] = 1\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "for i in range(cycles):\n",
    "    scores = np.dot(X, w_mult) + w0_mult\n",
    "    dominance = softmax(scores)\n",
    "    err = dominance - Y_multi\n",
    "    dw1 = (1 / n_samples) * np.dot(X.T, err)\n",
    "    dw0 = (1 / n_samples) * np.sum(err, axis=0, keepdims=True)\n",
    "    w0_mult -= step_size * dw0\n",
    "    w_mult -= step_size * dw1\n",
    "\n",
    "print(\"softmax class probabilities:\")\n",
    "print(dominance)\n",
    "print(\"multiclass LR predictions:\")\n",
    "print(np.argmax(dominance, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bc015c",
   "metadata": {},
   "source": [
    "## Known limitations\n",
    "\n",
    "- Fixed learning rate (no scheduling or adaptivity).\n",
    "- No regularization (L1/L2 not implemented).\n",
    "- Stopping is based on a fixed iteration count, not a convergence test.\n",
    "- No explicit train/validation split or evaluation metrics beyond basic sanity checks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
