{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce4417a",
   "metadata": {},
   "source": [
    "# Linear Regression from Scratch\n",
    "\n",
    "Quick reference for plain linear regression, gradient descent, closed form, and a simple ridge variant. Focus is on behavior and assumptions, not performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f39c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db66f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression: y = w0 + w1 x\n",
    "# Assume y = f(x; w) + eps with eps ~ N(0, sigma^2) → MSE is the NLL\n",
    "# MSE still works without Gaussian noise; Gaussian just justifies it\n",
    "# Logistic regression analog: Bernoulli likelihood → log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb41a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "# - Hypothesis: linear (or polynomial) functions of x\n",
    "# - Loss: MSE minimized by gradient descent; quadratic → smooth/convex\n",
    "# - Step size matters: too large can overshoot\n",
    "# - Polynomial fits can overfit quickly; ridge/L2 can tame large weights\n",
    "# - Closed form exists (normal equations); GD shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8ffd9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reminders\n",
    "# - Basis can be [1, x, x^2, ...]; higher degrees increase variance\n",
    "# - Squared loss is convex/smooth; GD converges with reasonable step size (Lipschitz)\n",
    "# - Normal equations give the closed form; think projection of y onto span(X)\n",
    "# - Small eigenvalues → ill-conditioned X^T X; ridge adds λ to stabilize and reduce variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4c05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator variance is 1/curvature (eigenvalue) along each eigen-direction of X^T X\n",
    "# Small eigenvalues = flat directions → large variance; ridge adds λ to lift them and reduce sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f63ffe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Values\n",
      "0.0 1.1111111111111112 2.2222222222222223 3.3333333333333335 4.444444444444445 5.555555555555555 6.666666666666667 7.777777777777779 8.88888888888889 10.0\n",
      "Predicted values\n",
      "0.5915961892133562 4.181580699028643 7.7715652088439295 11.361549718659218 14.951534228474504 18.54151873828979 22.131503248105076 25.721487757920364 29.31147226773565 32.90145677755093\n",
      "MSE (Gradient Descent): 1.524966\n"
     ]
    }
   ],
   "source": [
    "#Random guess at step size\n",
    "np.random.seed(0)\n",
    "X = np.linspace(0,10,10)\n",
    "Y = 3*X + 2 + np.random.normal(0,0.5,size=len(X))\n",
    "\n",
    "w0 = 0\n",
    "w1 = 0\n",
    "def prediction(X):\n",
    "    return w1*X + w0\n",
    "\n",
    "step_size = 0.001\n",
    "cycles = 100\n",
    "n_samples = len(X)\n",
    "for _ in range(cycles):\n",
    "    y_pred = prediction(X)\n",
    "    dw0 = (2/n_samples)*np.sum(y_pred - Y)\n",
    "    dw1 = (2/n_samples)*np.sum(X*(y_pred - Y))\n",
    "    w0 -= step_size*dw0\n",
    "    w1 -= step_size*dw1\n",
    "print(\"Actual Values\")\n",
    "print(*X)\n",
    "print(\"Predicted values\")\n",
    "print(*prediction(X))\n",
    "mse_gd = np.mean((Y - prediction(X))**2)\n",
    "print(f\"MSE (Gradient Descent): {mse_gd:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f2a97b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lipschitz Constant (L): 71.8030\n",
      "Calculated Step Size: 0.0139\n",
      "Final Weights: w0=2.7510, w1=2.9236\n",
      "MSE: 0.174109\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For MSE Loss: L = max eigenvalue of (2/n * X_augmented.T @ X_augmented)\n",
    "X_aug = np.column_stack((np.ones(len(X)),X))\n",
    "Hessian = (2/len(X))*(X_aug.T @ X_aug)\n",
    "eigenvalues = np.linalg.eigvals(Hessian)\n",
    "L = np.max(eigenvalues)\n",
    "step_size = 1 / L\n",
    "\n",
    "w0, w1 = 0, 0\n",
    "cycles = 1000\n",
    "def prediction(X, w0, w1):\n",
    "    return w1*X + w0\n",
    "\n",
    "for _ in range(cycles):\n",
    "    y_pred = prediction(X, w0, w1)\n",
    "    dw0 = (2/len(X))*np.sum(y_pred - Y)\n",
    "    dw1 = (2/len(X))*np.sum(X*(y_pred - Y))\n",
    "    w0 -= step_size*dw0\n",
    "    w1 -= step_size*dw1\n",
    "\n",
    "print(f\"Lipschitz Constant (L): {L:.4f}\")\n",
    "print(f\"Calculated Step Size: {step_size:.4f}\")\n",
    "print(f\"Final Weights: w0={w0:.4f}, w1={w1:.4f}\")\n",
    "print(f\"MSE: {np.mean((Y - prediction(X, w0, w1))**2):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a59832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values for degree 0: [3.15131527 3.15131527 3.15131527 3.15131527 3.15131527 3.15131527\n",
      " 3.15131527 3.15131527 3.15131527 3.15131527]\n",
      "MSE for degree 0: 289.364625\n",
      "Predicted values for degree 1: [ 0.59159619  4.1815807   7.77156521 11.36154972 14.95153423 18.54151874\n",
      " 22.13150325 25.72148776 29.31147227 32.90145678]\n",
      "MSE for degree 1: 1.524966\n",
      "Predicted values for degree 2: [-1.15754584e+55 -1.05933110e+57 -3.99991788e+57 -8.83333579e+57\n",
      " -1.55595848e+58 -2.41786650e+58 -3.46905763e+58 -4.70953188e+58\n",
      " -6.13928924e+58 -7.75832971e+58]\n",
      "MSE for degree 2: 1413151835874182194421437278450108850661531190643686600948241143319558619148900396884729747894618764346411324069117952.000000\n"
     ]
    }
   ],
   "source": [
    "#nth order regression\n",
    "#Random guess at step size\n",
    "step_size_poly = 0.00001\n",
    "cycles_poly = 100\n",
    "mse_poly = []\n",
    "for n in range(3):\n",
    "    w = np.zeros(n+1)\n",
    "    X_poly = np.column_stack([X**i for i in range(n+1)])\n",
    "    def predictor_n(X):\n",
    "        return np.dot(X_poly,w)\n",
    "    for _ in range(cycles):\n",
    "        y_pred = predictor_n(X_poly)\n",
    "        error = y_pred - Y\n",
    "        dw = (2/n_samples)*np.dot(X_poly.T,error)\n",
    "        w -= step_size*dw\n",
    "    print(f\"Predicted values for degree {n}:\", predictor_n(X_poly))\n",
    "    mse = np.mean((Y - predictor_n(X_poly))**2)\n",
    "    print(f\"MSE for degree {n}: {mse:.6f}\")\n",
    "    mse_poly.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8352c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed form solution\n",
      "MSE (Closed Form): 0.174109\n"
     ]
    }
   ],
   "source": [
    "#Closed-form \n",
    "#Add a dummy column to X for accomodating the +2 in Y = 3X+2\n",
    "def closed_form_W(X,Y):\n",
    "    return np.array((np.linalg.inv(X.T @ X)) @ X.T @ Y)\n",
    "X_closed = np.column_stack((np.ones(len(X)),X))\n",
    "W = closed_form_W(X_closed,Y)\n",
    "print(\"Closed form solution\")\n",
    "mse_closed = np.mean((Y - X_closed@W)**2)\n",
    "print(f\"MSE (Closed Form): {mse_closed:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "421362ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights without Ridge: [2.75181763 2.92343879]\n",
      "Weights with Ridge (L2):      [4.05865285 2.66207175]\n",
      "MSE without ridge: 0.174109\n",
      "MSE with ridge: 0.869887\n"
     ]
    }
   ],
   "source": [
    "X_closed_ridge = np.column_stack((np.ones(len(X)),X))\n",
    "\n",
    "def closed_form_ridge(X, Y, l2_penalty):\n",
    "    n_features = X.shape[1]\n",
    "    I = np.eye(n_features)\n",
    "    I[0,0] = 0\n",
    "    A = (X.T @ X) + (l2_penalty * I)\n",
    "    return np.linalg.inv(A) @ X.T @ Y\n",
    "\n",
    "W_no_ridge = closed_form_ridge(X_closed_ridge, Y, l2_penalty=0)\n",
    "W_ridge = closed_form_ridge(X_closed_ridge, Y, l2_penalty=10.0)\n",
    "\n",
    "print(\"Weights without Ridge:\", W_no_ridge)\n",
    "print(\"Weights with Ridge (L2):     \", W_ridge)\n",
    "mse_no_ridge = np.mean((Y - X_closed_ridge@W_no_ridge)**2)\n",
    "mse_ridge = np.mean((Y - X_closed_ridge@W_ridge)**2)\n",
    "print(f\"MSE without ridge: {mse_no_ridge:.6f}\")\n",
    "print(f\"MSE with ridge: {mse_ridge:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
